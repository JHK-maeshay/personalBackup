{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4-06-gan.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP7OwQ6GLQ2MNrc/MTl4nz3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["GAN의 아이디어에 대해 이해했다면, 이제 본격적으로 GAN을 파헤쳐볼 차례다. 본 포스팅에서는 우리에게 가장 익숙한 mnist 이미지를 생성하는 모델을 기준으로 예시를 들겠다. 먼저 이미지를 판별하는 Discriminator(이하 D)는 CNN 판별기처럼 구성할 수 있다. 그래서 이해하기에 매우 쉽다. 하지만 Generator(이하 G)를 구성하는 것은 새로운 아이디어가 도입된다.\n","\n","+ G는 random한 noise를 생성해내는 vector z를 input으로 하며\n","\n","+ D가 판별하고자 하는 input image(여기서는 28X28의 mnist 이미지)를 output으로 하는 neural network unit이라고 할 수 있다\n","\n","이렇게 GAN의 코어가 되는 모델은 D와 G 두 가지이다. 학습 과정에서는 실제 mnist 이미지, Real Image를 D로 하여금 '진짜'라고 학습시키는 1번 과정, 그리고 vector z와 G에 의해 생성된 Fake Image를 '가짜'라고 학습시키는 2번과정으로 나뉜다. 여기서 유의할 점은 D가 두번 학습되고 G는 1번 학습되는 것이 아니라, 1번 과정에서의 Real Image와 Fake Image를 D의 x input으로 합쳐서 학습한다는 것이다.\n"],"metadata":{"id":"zFmAn2xrlxD3"}},{"cell_type":"code","execution_count":46,"metadata":{"id":"OstUI_7Alm2O","executionInfo":{"status":"ok","timestamp":1649248071349,"user_tz":-540,"elapsed":5,"user":{"displayName":"김정훈","userId":"00475614692489779848"}}},"outputs":[],"source":["#실행할필요없음\n","\n","def train_D(self):\n","  \"\"\"\n","  train Discriminator\n","  \"\"\"\n","\n","  # Real data\n","  real = self.data.get_real_sample()\n","\n","  # Generated data\n","  z = self.data.get_z_sample(self.batch_size)\n","  generated_images = self.gan.G.predict(z)\n","\n","  # labeling and concat generated, real images\n","  x = np.concatenate((real, generated_images), axis=0)\n","  y = [0.9] * self.batch_size + [0] * self.batch_size\n","\n","  # train discriminator\n","  self.gan.D.trainable = True\n","  loss = self.gan.D.train_on_batch(x, y)\n","\n","  return loss\n","\n","def train_G(self):\n","  \"\"\"\n","  train Generator\n","  \"\"\"\n","\n","  # Generated data\n","  z = self.data.get_z_sample(self.batch_size)\n","\n","  # labeling\n","  y = [1] * self.batch_size\n","\n","  # train generator\n","  self.gan.D.trainable = False\n","  loss = self.gan.GD.train_on_batch(z, y)\n","  return loss"]},{"cell_type":"markdown","source":["+ train_D는 D를 학습하는 부분\n","\n","+  train_G는 D(G(z))에서 G를 학습하는 부분\n","\n","D.trainable을 사용하여, 위에서 설명한 대로 D는 한 번만 학습되도록 구현하였다. 코드에서 D(G(z))에서 D의 학습을 False로 한다면, 결국 G만 학습이 된다. 눈여겨 보아야 할 부분은 'x = np.concatenate((real, generated_images), axis=0)' 이다. 이 부분을 통해 진짜이미지와 가짜이미지를 D에게 한번에 학습시킨다.\n","\n","GAN 프레임 워크는 코어가 되는 두 개의 모델의 학습에 따라 진행된다. D의 목표는 Real, 혹은 Fake 이미지를 제대로 분류해내는 것이다. 그리고 G의 임무는 완벽하게 D가 틀리도록 하는 것이다. 그래서 두 코어 모델의 Loss 지표는 반대가 되며, 이 때문에도 '적대적' 모델로 불린다.\n","\n","$$min_G max_D V(D,G) = E_{x\\tilde{}p_{data}(x)}[logD(x)]+E_{z\\tilde{}p_z (z)}[log(1-D(G(z)))]$$\n","\n","https://arxiv.org/pdf/1406.2661.pdf"],"metadata":{"id":"fFpihPstmfkX"}},{"cell_type":"markdown","source":["GAN 모델은 일반적인 머신 러닝, 혹은 딥 러닝 모델과는 달리 명확한 평가의 기준이 없다. Loss는 단지 학습을 위한 오토 파라미터의 구실을 하는 셈이고, 실제적인 Loss를 나타내거나 Accuracy와 같은 기준이 되는 명확한 평가지표가 존재하지 않는다. 이미지를 생성하는 GAN의 경우, 사람의 육안으로 결과물을 평가할 수 있을 뿐이다.\n","\n","\n","Loss함수를 정의하고 이를 최적화 할때, 실제 환경에서는 생각보다 G의 초기 성능이 안좋다. 그래서 D(G(z))가 0에 가깝게 되는데, 원론적인 수식으로 적용하면 학습이 잘 안된다. 그래서 약간의 테크닉을 써서 수식을 살짝 바꾼다. (역시 수식에 대한 자세한 내용, 연구적인 내용에 관심이 있는 사람이라면 논문을 참고하자) 또한 D : G의 학습 비율은 1 : 5 와 같은 형태로 불균형하게 하는것이 일반적인듯 하다. D가 G에 비해 너무 정확하다면, G의 gradient가 vanishing되는 문제가 생기기도 하고, 반대의 경우도 생긴다. 어쨌든 이러한 문제점을 해결하기 위해 학습 비율을 조정을 하면 해결된다고 한다. 실제 코드를 돌려보면, D, G의 iterate를 다른 비율로 학습하는 것이 훨씬 학습이 잘된다.\n","\n","\n","GAN은 D, G 그리고 Noise에 대한 함수와 네트워크 구성을 자유롭게 하면서, GAN의 advanced 모델들을 구현해보는 쏠쏠한 재미가 있다. 그러한 모델들을 구현해보기 전에, 우선 기초가 되는 Gaussian 분포 생성과 mnist 이미지 생성에 대한 튜토리얼을 진행해보는 것을 권장한다."],"metadata":{"id":"ITrjLobgotYd"}},{"cell_type":"code","source":["#import argparse\n","#jupyter에서 argparse쓰면 에러난다!!!\n","import easydict\n","\n","import numpy as np\n","\n","from keras.models import Model, Sequential\n","from keras.layers.core import Reshape, Dense, Dropout, Flatten\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.layers.convolutional import Conv2D, MaxPooling2D, UpSampling2D\n","#from keras.layers.normalization import BatchNormalization\n","from tensorflow.keras.layers import BatchNormalization\n","from keras.datasets import mnist\n","#from keras.optimizers import Adam\n","from tensorflow.keras.optimizers import Adam\n","from keras import initializers\n","from keras import backend as K\n","\n","\n","K.set_image_data_format('channels_first')\n","\n","\n","class Data:\n","    \"\"\"\n","    Define dataset for training GAN\n","    \"\"\"\n","    def __init__(self, batch_size, z_input_dim):\n","        # load mnist dataset\n","        # 이미지는 보통 -1~1 사이의 값으로 normalization : generator의 outputlayer를 tanh로\n","        (X_train, y_train), (X_test, y_test) = mnist.load_data()\n","        self.x_data = ((X_train.astype(np.float32) - 127.5) / 127.5)\n","        self.x_data = self.x_data.reshape((self.x_data.shape[0], 1) + self.x_data.shape[1:])\n","        self.batch_size = batch_size\n","        self.z_input_dim = z_input_dim\n","\n","    def get_real_sample(self):\n","        \"\"\"\n","        get real sample mnist images\n","\n","        :return: batch_size number of mnist image data\n","        \"\"\"\n","        return self.x_data[np.random.randint(0, self.x_data.shape[0], size=self.batch_size)]\n","\n","    def get_z_sample(self, sample_size):\n","        \"\"\"\n","        get z sample data\n","\n","        :return: random z data (batch_size, z_input_dim) size\n","        \"\"\"\n","        return np.random.uniform(-1.0, 1.0, (sample_size, self.z_input_dim))\n","\n","\n","class GAN:\n","    #predict 에러 해결\n","    def predict(self, x):\n","        x = self.forward(x)\n","        return x\n","\n","    def __init__(self, learning_rate, z_input_dim):\n","        \"\"\"\n","        init params\n","\n","        :param learning_rate: learning rate of optimizer\n","        :param z_input_dim: input dim of z\n","        \"\"\"\n","        self.learning_rate = learning_rate\n","        self.z_input_dim = z_input_dim\n","        self.D = self.discriminator()\n","        self.G = self.generator()\n","        self.GD = self.combined()\n","\n","    def discriminator(self):\n","        \"\"\"\n","        define discriminator\n","        \"\"\"\n","        D = Sequential()\n","        D.add(Conv2D(256, (5, 5),\n","                     padding='same',\n","                     input_shape=(1, 28, 28),\n","                     kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n","        D.add(LeakyReLU(0.2))\n","        D.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n","        D.add(Dropout(0.3))\n","        D.add(Conv2D(512, (5, 5), padding='same'))\n","        D.add(LeakyReLU(0.2))\n","        D.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n","        D.add(Dropout(0.3))\n","        D.add(Flatten())\n","        D.add(Dense(256))\n","        D.add(LeakyReLU(0.2))\n","        D.add(Dropout(0.3))\n","        D.add(Dense(1, activation='sigmoid'))\n","\n","        adam = Adam(lr=self.learning_rate, beta_1=0.5)\n","        D.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n","        return D\n","\n","    def generator(self):\n","        \"\"\"\n","        define generator\n","        \"\"\"\n","        G = Sequential()\n","        G.add(Dense(512, input_dim=self.z_input_dim))\n","        G.add(LeakyReLU(0.2))\n","        G.add(Dense(128 * 7 * 7))\n","        G.add(LeakyReLU(0.2))\n","        G.add(BatchNormalization())\n","        G.add(Reshape((128, 7, 7), input_shape=(128 * 7 * 7,)))\n","        G.add(UpSampling2D(size=(2, 2)))\n","        G.add(Conv2D(64, (5, 5), padding='same', activation='tanh'))\n","        G.add(UpSampling2D(size=(2, 2)))\n","        G.add(Conv2D(1, (5, 5), padding='same', activation='tanh'))\n","\n","        adam = Adam(lr=self.learning_rate, beta_1=0.5)\n","        G.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n","        return G\n","\n","    def combined(self):\n","        \"\"\"\n","        defien combined gan model\n","        \"\"\"\n","        G, D = self.G, self.D\n","        D.trainable = False\n","        GD = Sequential()\n","        GD.add(G)\n","        GD.add(D)\n","\n","        adam = Adam(lr=self.learning_rate, beta_1=0.5)\n","        GD.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n","        D.trainable = True\n","        return GD\n","\n","\n","class Model:\n","    #predict 에러 해결\n","    def predict(self, x):\n","        x = self.forward(x)\n","        return x\n","\n","    def __init__(self, args):\n","        self.epochs = args.epochs\n","        self.batch_size = args.batch_size\n","        self.learning_rate = args.learning_rate\n","        self.z_input_dim = args.z_input_dim\n","        self.data = Data(self.batch_size, self.z_input_dim)\n","\n","        # the reason why D, G differ in iter : Generator needs more training than Discriminator\n","        self.n_iter_D = args.n_iter_D\n","        self.n_iter_G = args.n_iter_G\n","        self.gan = GAN(self.learning_rate, self.z_input_dim)\n","        self.d_loss = []\n","        self.g_loss = []\n","\n","        # print status\n","        batch_count = self.data.x_data.shape[0] / self.batch_size\n","        print('Epochs:', self.epochs)\n","        print('Batch size:', self.batch_size)\n","        print('Batches per epoch:', batch_count)\n","        print('Learning rate:', self.learning_rate)\n","        print('Image data format:', K.image_data_format())\n","\n","    def fit(self):\n","        for epoch in range(self.epochs):\n","\n","            # train discriminator by real data\n","            dloss = 0\n","            for iter in range(self.n_iter_D):\n","                dloss = self.train_D()\n","\n","            # train GD by generated fake data\n","            gloss = 0\n","            for iter in range(self.n_iter_G):\n","                gloss = self.train_G()\n","\n","            # print loss data\n","            print('Discriminator loss:', str(dloss))\n","            print('Generator loss:', str(gloss))\n","\n","    def train_D(self):\n","\n","        #train Discriminator\n","\n","        # Real data\n","        real = self.data.get_real_sample()\n","\n","        # Generated data\n","        z = self.data.get_z_sample(self.batch_size)\n","        generated_images = self.gan.G.predict(z)\n","\n","        # labeling and concat generated, real images\n","        x = np.concatenate((real, generated_images), axis=0)\n","        y = [0.9] * self.batch_size + [0] * self.batch_size\n","\n","        # train discriminator\n","        self.gan.D.trainable = True\n","        loss = self.gan.D.train_on_batch(x, y)\n","        return loss\n","\n","    def train_G(self):\n","\n","        #train Generator\n","\n","        # Generated data\n","        z = self.data.get_z_sample(self.batch_size)\n","\n","        # labeling\n","        y = [1] * self.batch_size\n","\n","        # train generator\n","        self.gan.D.trainable = False\n","        loss = self.gan.GD.train_on_batch(z, y)\n","        return loss\n","\n","def main():\n","    # set hyper parameters\n"," \n","    args = easydict.EasyDict({\n","        \n","        \"batch_size\": 128,\n"," \n","        \"epochs\": 200,\n"," \n","        \"learning_rate\": 0.0002,\n"," \n","        \"z_input_dim\": 100,\n"," \n","        \"n_iter_D\": 1,\n","\n","        \"n_iter_G\": 5,\n"," \n","        \"unit\": 1000\n"," \n","      })\n","\n","    \n","    \"\"\"\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('--batch_size', type=int, default=128,\n","                        help='Batch size for networks')\n","    parser.add_argument('--epochs', type=int, default=200,\n","                        help='Epochs for the networks')\n","    parser.add_argument('--learning_rate', type=float, default=0.0002,\n","                        help='Learning rate')\n","    parser.add_argument('--z_input_dim', type=int, default=100,\n","                        help='Input dimension for the generator.')\n","    parser.add_argument('--n_iter_D', type=int, default=1,\n","                        help='training iteration for D')\n","    parser.add_argument('--n_iter_G', type=int, default=5,\n","                        help='training iteration for G')\n","    args = parser.parse_args()\n","    \"\"\"\n","\n","    # run model\n","    model = Model(args)\n","    model.fit()\n","\n","main()\n","\n","\"\"\"\n","if __name__ == '__main__':\n","    main()\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"NIhfoyS_qXEm","executionInfo":{"status":"error","timestamp":1649249906994,"user_tz":-540,"elapsed":4671,"user":{"displayName":"김정훈","userId":"00475614692489779848"}},"outputId":"0b61b00a-b077-4e31-8cce-c482e10db3b9"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(Adam, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 200\n","Batch size: 128\n","Batches per epoch: 468.75\n","Learning rate: 0.0002\n","Image data format: channels_first\n"]},{"output_type":"error","ename":"UnimplementedError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-40e63e1612ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \"\"\"\n","\u001b[0;32m<ipython-input-1-40e63e1612ba>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;31m# run model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-40e63e1612ba>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mdloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                 \u001b[0mdloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;31m# train GD by generated fake data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-40e63e1612ba>\u001b[0m in \u001b[0;36mtrain_D\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;31m# Generated data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_z_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mgenerated_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;31m# labeling and concat generated, real images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node 'sequential_1/conv2d_2/Conv2D' defined at (most recent call last):\n    File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 499, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n      handler_func(fileobj, events)\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 452, in _handle_events\n      self._handle_recv()\n    File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 481, in _handle_recv\n      self._run_callback(callback, msg)\n    File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 431, in _run_callback\n      callback(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n      return self.dispatch_shell(stream, msg)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n      handler(stream, idents, msg)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n      user_expressions, allow_stdin)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n      if self.run_code(code, result):\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-1-40e63e1612ba>\", line 257, in <module>\n      main()\n    File \"<ipython-input-1-40e63e1612ba>\", line 255, in main\n      model.fit()\n    File \"<ipython-input-1-40e63e1612ba>\", line 168, in fit\n      dloss = self.train_D()\n    File \"<ipython-input-1-40e63e1612ba>\", line 188, in train_D\n      generated_images = self.gan.G.predict(z)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1982, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1801, in predict_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1790, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1783, in run_step\n      outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1751, in predict_step\n      return self(x, training=False)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py\", line 374, in call\n      return super(Sequential, self).call(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\", line 452, in call\n      inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\", line 589, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/layers/convolutional.py\", line 248, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/layers/convolutional.py\", line 240, in convolution_op\n      name=self.__class__.__name__)\nNode: 'sequential_1/conv2d_2/Conv2D'\nThe Conv2D op currently only supports the NHWC tensor format on the CPU. The op was given the format: NCHW\n\t [[{{node sequential_1/conv2d_2/Conv2D}}]] [Op:__inference_predict_function_599]"]}]}]}