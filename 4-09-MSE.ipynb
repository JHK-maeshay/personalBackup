{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4-09-MSE.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPVd5wwrMsxipOWxjoq1wm4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["+ **선형 회귀 모델의 예측**\n","\n","$$\\hat{y} = \\boldsymbol{\\theta}_0+\\boldsymbol{\\theta}_1x_1+\\boldsymbol{\\theta}_2x_2+  ... +\\boldsymbol{\\theta}_nx_n$$\n","\n","*   $\\hat{y}$은 예측값입니다.\n","*   $n$은 특성의 수입니다.\n","*   $x_i$는 $i$번째 특성값입니다. \n","*   $\\boldsymbol{\\theta}_j$는 $j$번째 모델 파라미터입니다 (편향 $\\boldsymbol{\\theta}_0$과 특성의 가중치 $\\boldsymbol{\\theta}_1, \\boldsymbol{\\theta}_2, ..., \\boldsymbol{\\theta}_n$을 포함합니다). \n"],"metadata":{"id":"G4S21XWVNFNe"}},{"cell_type":"markdown","source":["+ **선형 회귀 모델의 예측(벡터 형태)**\n","\n","$$\\hat{y} = h_\\boldsymbol{\\theta}(x) = \\boldsymbol{\\theta}  \\boldsymbol{\\cdot} x$$\n","\n","*   $\\boldsymbol{\\theta}$는 편향 $\\boldsymbol{\\theta}_0$과 $\\boldsymbol{\\theta}_1$에서 $\\boldsymbol{\\theta}_n$까지의 특성 가중치를 담은 모델의 파라미터 벡터입니다. \n","*   $x$은 $x_0$에서 $x_n$까지 담은 샘플의 특성 벡터입니다. $x_0$는 항상 1로써, 벡터표현으로 모델 파라미터와 특성을 모두 표현하기 위해 편향 $\\boldsymbol{\\theta}_0$에 가상의 특성 $x_0=1$이 곱해졌다고 생각하시면 됩니다. \n","*   $\\boldsymbol{\\theta} \\boldsymbol{\\cdot} x$는 벡터 $\\boldsymbol{\\theta}$와 $x$의 점곱입니다. 이는 $\\boldsymbol{\\theta}_0x_0+\\boldsymbol{\\theta}_1x_1+\\boldsymbol{\\theta}_2x_2+ ...+\\boldsymbol{\\theta}_nx_n$와 같습니다.\n","*  $h_\\boldsymbol{\\theta}$는 모델 파라미터 $\\boldsymbol{\\theta}$를 사용한 가설(hypothesis)함수 입니다."],"metadata":{"id":"X2HCM4mPNxDe"}},{"cell_type":"markdown","source":["+ **평균 제곱 오차 (MSE)**\n","\n","\n","$$MSE(X,h_\\boldsymbol{\\theta}) = \\frac{1}{M}\\sum_{i=1}^{m}(\\boldsymbol{\\theta}^Tx^{(i)}-y^{(i)})^2$$"],"metadata":{"id":"5-2XG_8INxlZ"}},{"cell_type":"markdown","source":["***\n","$\\theta x = \\hat{y} = y \\iff MSE = 0$가 되기 위한 $\\theta$의 값을 구한다\n","***"],"metadata":{"id":"0pa_zabwOIRv"}},{"cell_type":"markdown","source":["+ **정규 방정식**\n","\n","$$\\hat{\\boldsymbol{\\theta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$$\n","\n","*   $\\hat{\\boldsymbol{\\theta}}$은 비용 함수를 최소화하는 ${\\boldsymbol{\\theta}}$ 값\n","*   $y$는 $y^{(1)}$부터 $y^{(m)}$까지 포함하는 타켓벡터"],"metadata":{"id":"fzqVg3_RNx3K"}},{"cell_type":"markdown","source":["1. Least Squares 추정치를 계산하기 위해서는 우선 아래의 형태의 error의 L2 Norm에 대한 제곱을 편미분을 세타에 대해서 수행해야 한다.\n","$$||y - X \\theta ||^2$$\n","\n","2. 여기서 중요한 것은 세타가 벡터라는 것이다. 그러면 그것의 편미분을 구한다는 것은 각각의 세타에 대해서 따로 미분을 해주고, 그것을 0으로 둔다는 것인데,,, 재미난 것은 아래와 같이 하나의 수식으로 모든 경우의 세타에 대해서 한번에 표현이 된다는 것이다.\n","\n","$$(-2)X^T (y - X \\hat{\\theta}) = 0$$\n","\n","\n","\n","***\n","$X = \\begin{bmatrix} 1 & 2 & 3 \\end{bmatrix}\n","$\n",", \n","$\\hat{\\theta}=\\begin{bmatrix} \\theta_1 & \\theta_2 & \\theta_3 \\end{bmatrix}\n","$\n",", \n","$y=6$\n","이라고 가정해보자\n","$$∇||y - X \\hat{\\theta} ||^2$$\n","$$=∇(y - (\\theta_1+2\\theta_2+3\\theta_3))^2$$\n","$$=-2(y - (\\theta_1+2\\theta_2+3\\theta_3) ⋅ 1$$\n","$$-2(y - (\\theta_1+2\\theta_2+3\\theta_3) ⋅ 2$$\n","$$-2(y - (\\theta_1+2\\theta_2+3\\theta_3) ⋅ 3$$\n","$$=-2(1+2+3)(\\theta_1+2\\theta_2+3\\theta_3)$$\n","$$=-2X^T(y - X \\hat{\\theta})$$\n","***\n","\n","\n","3. 우리는 p개의 모르는 세타라는 파라미터를 가지고 있으면서, p개의 선형 방정식을 가지고 있게 되는 상태가 된다. 여기서 $X^TX = p × p$ 행렬이라는 것을 명심하자. 그러면 차원이 맞아 떨어질 것이다. 위의 방정식의 차원을 정리하면 $$(p × p) × (p × 1) = (p × n) × (n × 1)$$즉, 양변 모두 $(p × 1)$이 된다. p차원의 벡터가 된다. $X^TX$의 각 원소들이 의미하는 것은 X의 열들의 scalar product 즉, 내적 값이 된다. 따라서, Normal Equation을 푸는 것은 X의 열들이 orthogonal 할 때, 특히나 단순해진다."],"metadata":{"id":"GZtCcgZ3PA7c"}},{"cell_type":"markdown","source":["\n","4. 모든 열들이 서로 orthogonal 한 상태인 것이다. 그렇게 되면 X^T x X는 역행렬을 가지게 된다(invertible). 그러한 경우에, Least Squares 솔루션은 유일하게 되고(unique) 아래와 같이 바로 계산이 된다.\n","$$-2X^T(y - X \\hat{\\theta})$$\n","$$X^TX \\hat{\\theta}=X^Ty$$\n","$$\\hat{\\theta} = (X^TX)^{-1}X^Ty$$"],"metadata":{"id":"819NOF0JWPRi"}},{"cell_type":"markdown","source":["5. 위의 수식은 앞에서 말한 **Orthogonality 가정이 필요**하다는 것을 잘 기억해야 한다. 만약에 Correlation이 입력 변수 간에 존재하게 되면 위의 관계는 성립하지 않고, 앞에서 다룬 방정식을 풀어야 한다. 하지만, 보통 **이 가정은 선형 모델에 포함되어 있기 때문에**, 위의 수식으로 풀 수 있는 경우가 대부분이 될 것이다."],"metadata":{"id":"V7dUT6RWW0Ow"}},{"cell_type":"markdown","source":["**참고\n","\n","$X = p × n$\n","\n","$y = n × 1$\n","\n","$\\theta = p × 1$\n","\n","$X=\\begin{bmatrix} 1 & 2 & 3 \\\\ 2 & 3 & 4 \\\\ 3 & 4 & 5 \\\\ 4 & 5 & 6 \\\\ 5 & 6 & 7 \\end{bmatrix} y=\\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{bmatrix} \\theta = \\begin{bmatrix}w_1\\\\w_2\\\\w_3\\end{bmatrix}\n","$\n"],"metadata":{"id":"dyk77qW7Xnp9"}},{"cell_type":"markdown","source":["+ **비용 함수의 그레이디언트 벡터**\n","\n","$\n","\\dfrac{\\partial}{\\partial \\boldsymbol{\\theta}} \\text{MSE}(\\boldsymbol{\\theta})\n"," = \\dfrac{2}{m} \\mathbf{X}^T (\\mathbf{X} \\boldsymbol{\\theta} - \\mathbf{y})\n","$"],"metadata":{"id":"Mz-J74PQuMTm"}},{"cell_type":"markdown","source":["+ **경사 하강법의 스텝**\n","\n","$\n","\\boldsymbol{\\theta}^{(\\text{next step})} = \\boldsymbol{\\theta} - \\eta \\dfrac{\\partial}{\\partial \\boldsymbol{\\theta}} \\text{MSE}(\\boldsymbol{\\theta})\n","$"],"metadata":{"id":"zzGLnju-uN2q"}}]}